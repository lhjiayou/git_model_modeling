# -*- coding: utf-8 -*-
"""
Created on Sun Sep 18 19:58:12 2022

@author: 18721
"""

# 统计分析-回归分析与分类分析
# 统计建模的主要任务有二：预测与推断。
#预测任务就是根据特征来预测对应的输出y
#推断任务是分析特征对输出y的影响

#task-1 估计
# 经典线性模型(CLM, Classical Linear Model)与最小二乘估计(OLS)，广义线性模型(GLM)
# 在满足CLM假设的前提下，OLS估计是经典线性模型最优的参数估计法；

#task-2 检验
# 基于CLM假设与OLS估计，我们便可以对模型进行各种假设检验，包括参数显著性检验，模型显著性检验等等。   检验知识

#task-3误差分析
# 。某个假设不成立会给模型参数估计的准确性（无偏性）、稳定性（方差）以及假设检验带来多少影响呢？这就是模型设定误差分析需要研究的内容。

#task-4解决办法
# 若数据不满足CLM假设中的某个假设，需要找到对应的解决办法，

# 1. 回归模型总述
# 1.1 回归思想与一般回归模型
# 1.1.1 横截面数据
# 横截面数据外，还有时间序列数据以及面板数据

# 1.1.2 回归思想——条件均值建模
# 其实就是想知道条件分布的估计，但是在实际问题中，直接估计这个条件分布几乎是一件不可能的事，且我们也难以对分布进行解释与应用。
# 于是，我们退而求其次通过分布的一般数字特征对两者的关系进行推断，如条件分布的中心位置，形状，即考虑条件均值、条件方差
# 回归正是利用条件均值 𝐸(𝑦∣𝑥) 来刻画 𝑥 与 𝑦 的关系，回归建模的本质也正是“条件均值的建模”。
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns
from IPython.display import display

x=list()   
y=list()
for i in [1,2,3,4,5]:
    y_norm=stats.norm.rvs(i,1,20,random_state=i).tolist()
    y.extend(y_norm)      #每个i会生成20个正态分布数据
    x1=np.ones(20)*i
    x1=x1.tolist()
    x.extend(x1)   #每个i会重复20次

data={'x':x,'y':y}  
df=pd.DataFrame(data)  #由字典构建dataframe，100*2

sns.regplot(x='x',y='y',data=df)
plt.title('E(Y|X)')

# 1.1.3 一般回归模型
# 只要假定随机误差 𝑢 与 𝑥 不相关（这里可理解为其他影响 𝑦 的外生因素与内生因素 𝑥 不相关），我们就可以根据需要假定回归模型的具体形式。


# 1.2 线性回归模型
# 1.2.1 线性模型形式
# 回归分析主要研究如何有效地估计模型中的参数 𝛽̂ 𝑖 ，并利用模型进行推断与预测。

# 1.2.2 从简单线性回归到多元线性回归
# 设定 𝐸(𝑢|𝑥)=0 的存在暗含了在该模型中高中测验成绩、自主学习能力等因素与自变量大学测验水平无关，但这在实际问题中未必成立。而一旦它们存在相关性，就意味着模型假设不符合实际情况，模型估计的有效性与准确性也将受到影响。
#也就是说需要挖掘充分的特征
import statsmodels.api as sm
# 加载数据
gpa1=pd.read_stata('./data/gpa1.dta')

# 在数据集中提取自变量
X1=gpa1.ACT
X2=gpa1[['ACT','hsGPA']]
# 提取因变量
y=gpa1.colGPA

# 为自变量增添截距项
X1=sm.add_constant(X1)    #141*2
X1.columns  #Index(['const', 'ACT'], dtype='object')
X2=sm.add_constant(X2)    #141*3
X2.columns   #Index(['const', 'ACT', 'hsGPA'], dtype='object')
display(X2)

#利用OLS来实现模型的最小二乘拟合
# 拟合两个模型
gpa_lm1=sm.OLS(y,X1).fit()
gpa_lm2=sm.OLS(y,X2).fit()

# 输出两个模型的系数与对应p值
p1=pd.DataFrame(gpa_lm1.pvalues,columns=['pvalue'])
c1=pd.DataFrame(gpa_lm1.params,columns=['params'])
p2=pd.DataFrame(gpa_lm2.pvalues,columns=['pvalue'])
c2=pd.DataFrame(gpa_lm2.params,columns=['params'])
display(c1.join(p1,how='right'))  #其实要不要how的结果是一样的
#          params        pvalue
# const  2.402979  8.798591e-16
# ACT    0.027064  1.389927e-02
display(c2.join(p2,how='right'))
#          params    pvalue
# const  1.286328  0.000238
# ACT    0.009426  0.383297
# hsGPA  0.453456  0.000005
# 多个重要变量都纳入回归模型的重要性。总之，大家在此只需要知道：多元线性回归非常重要


# 2. 模型系数的估计方法——OLS估计及其性质
# 你这个线性回归模型里的参数是使用什么方法计算出来的呢？按照你这种方法计算出来的参数是否可靠呢？
# 它们又具备哪些统计性质呢？那么这一章，我们将学习线性回归中最常用、最经典的系数估计方法——普通最小二乘估计法(Ordinary Least Squares, OLS)
# 还有就是加权最小二乘估计
# 2.1 OLS估计的思想与原理
# 2.1.1 OLS估计的思想  就是残差平方和最小
# 2.1.2 OLS估计的求解  可以写成矩阵的形式
# 以gpa_lm2为例
## 手动计算系数的估计向量
X2_T=X2.values.T     
X_inv=np.linalg.inv(np.dot(X2_T,X2)) # 求矩阵乘积的逆矩阵
Xy=np.dot(X2_T,y.values)
beta_hat=np.dot(X_inv,Xy)
print('手动计算的系数向量为：')
print(beta_hat)

## 软件计算的系数向量
print('-----------------------------------')
print('软件计算的系数为：')
display(c2)

# 2.1.3 拟合优度
# TSS(Total sum of squares)，总平方和
# ESS(Explained sum of squares)，解释平方和

# RSS(Resiual sum of squares)，残差平方和
# 直观上RSS是一个可以度量拟合优度的量，因为残差平方和越小，意味着预测值与真实值之间的差距越小。
# 但是RSS的大小没有一个标准，它将随着样本量的增大而增大，因此单纯的RSS不是一个合格的衡量拟合优度的量。

# 这个时候我们可以从另一个角度去理解回归建模的意义。我们之所以想构建模型，是因为想找到造成 𝑦 值变化的因素，模型解释的变异占总变异的比例越多，
# 这个模型的解释力度就越大，模型的拟合优度也就越好。
# 拟合优度就有思路了。回归分析中最常用的拟合优度是R方=ESS/TSS=1-RSS/TSS
# 事实上三种平方和存在关系 𝑇𝑆𝑆=𝑅𝑆𝑆+𝐸𝑆𝑆 （大家可以尝试自己推导），这说明：总变异可以被拆分为解释了变异和未被解释的变异，残差平方和度量了“剩余信息”。
# 动手计算模型gpa_lm2的R方
TSS_gpa=np.sum(np.power(gpa1.colGPA-np.mean(gpa1.colGPA),2))
RSS_gpa=np.sum(np.power(gpa_lm2.resid,2))
gpa_lm2_R2=1-RSS_gpa/TSS_gpa
print('手动计算的R方为：{}'.format(gpa_lm2_R2))
print('-------------------------------------------------')
# 直接输出模型gpa_lm2的R方
gpa_lm2_R2=gpa_lm2.rsquared
print('软件计算的R方为：{}'.format(gpa_lm2_R2))   #但是比较小，说明效果不是很好


# 2.1.4 OLS估计的代数性质
#性质1： OLS估计预测残差之和为0；此外，这可以推出预测残差的均值也为0，即 𝑢̂ ¯=0 
print('简单回归模型的残差和（保留四位小数点）：{:.4f}'.format(sum(gpa_lm1.resid)))
print('多元回归模型的残差和（保留四位小数点）：{:.4f}'.format(sum(gpa_lm2.resid)))
#性质2：OLS估计的残差与参与回归的自变量不相关
# 定义计算残差的计算函数
from pylab import *
def de_mean(x):
    xmean = np.mean(x)
    return [xi - xmean for xi in x]
# 定义计算样本协方差的计算函数
def covariance(x, y):
    n = len(x)
    return dot(de_mean(x), de_mean(y)) / (n-1)
print('回归模型中残差与自变量hsGPA的样本协方差为（保留四位小数点）：{:.4f}'.format(covariance(gpa_lm2.resid,gpa1.hsGPA)))


# 2.2 经典线性模型假设下OLS估计的性质
# 2.2.1 经典线性模型假设-CLM假设(在计量经济学导论中有些)

# MLR.1 总体模型假设 𝑦=𝛽0+𝛽1𝑥1+𝛽2𝑥2+⋯+𝛽𝑘𝑥𝑘+𝑢  假定了我们正确地判断了因变量和自变量之间的关系——既正确设定了模型形式为上述的线性形式，又正确纳入了所有自变量。
#这是很难的，首先模型形式能够直接确定还是很难的，大部分非线性的就无法确定，另外特征是否选择的合适，没有漏掉也是很难的

 # MLR.2 随机误差条件均值零假设𝐸(𝑢∣𝑥1,⋯,𝑥𝑘)=0  意味着所有非自变量的其他因素都与自变量线性无关
 #但是如果我们没能成功挖掘所有的x，那么u中就会包含哪些没能挖掘出来的x，就可能是有问题的
 
 # MLR.3 随机抽样假设  𝑛 个来自上述总体的样本均为随机抽样样本，彼此之间相互独立
 #样本独立同分布的假设也是很难的
 
 # MLR.4 非完全共线性假设
 # 这些样本的所有自变量间不能存在有完全共线性，即不能存在某一自变量可由其余自变量进行线性表示的情况。
 #如果存在，那么这个特征就没有任何的意义，根本不需要
 
 # MLR.5 同方差假设 Var(𝑢∣𝑥1,⋯,𝑥𝑘)=𝜎2或者Var(𝑦∣𝑥1,⋯,𝑥𝑘)=𝜎2
 # 数据的波动程度不受自变量影响，不论 𝑥𝑖 如何变化，数据与样本条件均值的偏离程度都是恒定的。
 #同方差就是特征x的变化不会造成残差的方差变化，方差始终是定值
 
 # MLR.6 正态性假设
 # 随机误差 𝑢 在任何自变量 𝑥 已知的条件下服从正态分布 𝑢∣𝑥∼𝑁(0,𝜎2)
 # 这一假设实际上是MLR.2与MLR.5假设的升级版，即在随机误差 𝑢 的零条件期望与恒定条件方差的基础上，增加了一个服从条件正态分布的假设。
 
 # 以上六个假设是一种非常严格、理想化的假设，只有在这些假设成立的基础上我们才能对OLS估计在线性回归模型上的性质作进一步的研究。
 # 当然，实际的数据并不一定都能满足这些假设，     检验任务
 # 有关样本数据是否可以满足这些假设的识别检验、    改进方法
 # 不满足假设的后果以及改进方案，我们将在以后的章节学习。
 
 
 # 2.2.2 OLS估计的性质-最优的线性无偏估计
 # 总体回归函数  就是真实的模型，真实的映射关系
 # 样本回归函数  就是估计的模，根据有误差的数据估计得到的模型
 
 #OLS系数的均值
#  定理1. 在CLM假设MLR.1-MLR.4下， 𝛽̂  是 𝛽 的无偏估计，即
# 𝐸(𝛽̂ 𝑗)=𝛽𝑗,∀𝑗=0,1,⋯,𝑘 
# 无偏性意味着我们使用OLS进行多次试验后，估计出来的系数均值与参数的真实值是吻合的，这是一件激励人心的事

#OLS系数的方差
# 定理2. 在CLM假设MLR.1-MLR.5（增加了同方差假设MLR.5)下
# 标准差 sd(βj)=σ*sqrt(XTX)   因为这个σ不可能知道，因此这个值只是理论存在的，用标准误进行代替
# 标准误(standard error)   se(βj)=RSS/(n-k-1) *sqrt(XTX)

# 定理3. 在CLM假设MLR.1-MLR.5下， 𝜎̂ 2 是 𝜎2 的无偏估计，即
# 𝐸(𝜎̂ 2)=𝜎2  表明，在CLM假设下，我们上述对随机误差的方差的估计是“准确的”。

# 依旧以gpa_lm2模型为例
# 手动计算残差标准差的估计值，并比较python直接输出的结果

# 𝜎̂ 2的对比
## 手动计算
df=gpa_lm2.df_resid # 计算自由度141-2-1=138
sigma=RSS_gpa/df
print('手动计算的回归标准误：{}'.format(sigma))
## 软件输出
sigma2=gpa_lm2.scale
print('软件计算的回归标准误：{}'.format(sigma2))
print('-------------------------------------------------')

# 变量ACT系数的标准误se(βj)
## 手动计算
X2_T=X2.values.T
X_inv=np.linalg.inv(np.dot(X2_T,X2)) # 求矩阵乘积的逆矩阵
se_beta1=np.sqrt(sigma*X_inv[(1,1)])
print('手动计算的ACT系数标准误：{}'.format(se_beta1))
## 软件输出
se_beta1=gpa_lm2.bse[1]
print('软件计算的ACT系数标准误：{}'.format(se_beta1))


# 直接使用接口summary来直观的展示模型拟合的各种指标结果。
print(gpa_lm2.summary())


# OLS系数估计的最优线性无偏性
# 在所有无偏估计当中，OLS估计是最优的，因为有如下定理
# Gauss-Markov定理. 在CLM假设MLR.1-MLR.5下，在 𝛽 的所有线性无偏估计类当中，OLS估计的方差最小。即假设另有无偏估计 𝛽̃ 𝑗 ，若它可以表示为 𝑦𝑖 的线性组合，则必有
# Var(𝛽̂ 𝑗)<Var(𝛽̃ 𝑗)
# 值得注意的是，OLS只是在线性无偏估计中的方差最小，如果我们不追求估计的无偏性而只追求估计的稳定性（小方差），可以采用岭估计等有偏估计。


 # 定理4. 在CLM假设MLR.1-MLR.6下， 𝛽̂ 𝑗 服从正态分布𝛽̂ 𝑗∼𝑁(𝛽𝑗,Var(𝛽̂ 𝑗))
 #上式虽然可以变形为：𝛽̂ 𝑗−𝛽𝑗  /𝑠𝑑(𝛽̂ 𝑗)∼𝑁(0,1)
 #但是其实𝑠𝑑(𝛽̂ 𝑗)是未知的，并不能用正态分布进行检验
 #相反，应该𝛽̂ 𝑗−𝛽𝑗  /se(𝛽̂ 𝑗)∼𝑡𝑛−𝑘−1
 #基于这个t分布进行检验，就能够对模型检验，参数显著性检验，模型显著性检验